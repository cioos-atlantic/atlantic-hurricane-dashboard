{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting all the input variables\n",
    "\n",
    "storm_name = \"dorian\"\n",
    "storm_year = 2019\n",
    "storm_lowerLat= 40\n",
    "\n",
    "# How many days before / after the storm dates to get data for graphing\n",
    "dateExtension = 2\n",
    "\n",
    "focus_variable = \"wind_spd_avg\"\n",
    "var_units = \"(m s-1)\"\n",
    "\n",
    "# Time in hours after the first measurement of the storm to observe\n",
    "storm_time_offset = 8\n",
    "\n",
    "# Time in hours to go back for a station's measurement\n",
    "# Change to just have the minimum time be the start of the storm?\n",
    "storm_time_bounds = 1\n",
    "\n",
    "# Color map style to use for the output station data\n",
    "color_map = 'Blues'\n",
    "\n",
    "# Buoys to exclude (for demoing purposes these either give odd values or overlap with other markers)\n",
    "#exclude=  [\"sma_negl_cartwright_junction_nlqu0004\", \"SMA_halifax_fairview\", \"SMA_port_aux_basqes_wharf\"]\n",
    "exclude= []\n",
    "\n",
    "#interest_variables = [\"wind_spd\", \"wave_ht\", \"pressure\"]\n",
    "# Categories: Wind, Surface Waves, Pressure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting storm data and matching ERDDAP datasets around the storm (time and space)\n",
    "\n",
    "import tropycal.tracks as tracks\n",
    "import pandas as pd\n",
    "from erddapy import ERDDAP\n",
    "from datetime import timedelta\n",
    "import pytz\n",
    "\n",
    "basin = tracks.TrackDataset(basin='north_atlantic', source='ibtracs')\n",
    "storm = basin.get_storm((storm_name,storm_year))\n",
    "\n",
    "# Coordinate selector throws an error when using with Fiona but still generates the graph\n",
    "storm = storm.sel(lat=[storm_lowerLat,None])\n",
    "\n",
    "storm_dict = storm.interp().to_dict()\n",
    "\n",
    "# Some fields are left empty after interpolation, so need to only include ones that will have values\n",
    "storm_dict_cut = {\n",
    "    'date':storm_dict['date'],\n",
    "    'type':storm_dict['type'],\n",
    "    'lat':storm_dict['lat'],\n",
    "    'lon':storm_dict['lon'],\n",
    "}\n",
    "\n",
    "storm_df = pd.DataFrame.from_dict(storm_dict_cut)\n",
    "\n",
    "storm_df['lon'] = storm_df['lon'].apply(lambda x: x-360.0)\n",
    "\n",
    "start_date = pytz.utc.localize(storm_df.min()['date'])\n",
    "end_date = pytz.utc.localize(storm_df.max()['date'])\n",
    "\n",
    "storm_df['date'] = storm_df['date'].dt.tz_localize('UTC')\n",
    "print(storm_df['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "datasets = os.listdir(f'../station-data/{storm_year}_{storm_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Find most recent values for focus var given a time point\n",
    "selected_time=  start_date + timedelta(hours = storm_time_offset)\n",
    "print(selected_time)\n",
    "matching_points = pd.DataFrame()\n",
    "\n",
    "for dataset in datasets:\n",
    "    try:\n",
    "        buoy_data= pd.read_csv(f'../station-data/{storm_year}_{storm_name}/{dataset}')\n",
    "        buoy_data['time (UTC)'] = pd.to_datetime(buoy_data['time (UTC)'])\n",
    "        storm_buoy = buoy_data.loc[(buoy_data['time (UTC)'] < selected_time) & (buoy_data['time (UTC)'] >= selected_time - timedelta(hours = storm_time_bounds))]\n",
    "        # TODO Find out why it's not limiting to time\n",
    "        recent_row = storm_buoy.tail(1)\n",
    "        recent_row.insert(0, \"dataset\", dataset)\n",
    "        matching_points = pd.concat([matching_points, recent_row.loc[:]]).reset_index(drop=True)\n",
    "    except:\n",
    "        print(dataset)\n",
    "        print(\"couldn't find info for the given dataset for the given time\")\n",
    "matching_points.set_index('dataset')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mticker\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.colors as colors\n",
    "\n",
    "try:\n",
    "    import cartopy.feature as cfeature\n",
    "    from cartopy import crs as ccrs\n",
    "    from cartopy.mpl.gridliner import LONGITUDE_FORMATTER, LATITUDE_FORMATTER\n",
    "except:\n",
    "    warnings.warn(\"Warning: Cartopy is not installed in your python environment. Plotting functions will not work.\")\n",
    "\n",
    "proj = ccrs.Mercator(central_longitude=305, min_latitude=40, max_latitude=54)\n",
    "fig, ax = plt.subplots(subplot_kw=dict(projection=proj), figsize=(12,12))\n",
    "ax.set_extent([312 , 285, 40, 54], crs=ccrs.PlateCarree())\n",
    "ax.add_feature(cfeature.LAND, facecolor='0.8')\n",
    "ax.add_feature(cfeature.BORDERS, zorder=10)\n",
    "ax.add_feature(cfeature.COASTLINE, zorder=10)\n",
    "gl = ax.gridlines(crs=ccrs.PlateCarree(), linewidth=2, color='black', alpha=.5, linestyle='--', draw_labels=True)\n",
    "\n",
    "gl.xformatter = LONGITUDE_FORMATTER\n",
    "gl.yformatter = LATITUDE_FORMATTER\n",
    "\n",
    "map_storm_line = ax.plot(storm_df['lon'],storm_df['lat'], linestyle = 'dashed', linewidth =3, c= 'black', transform=ccrs.PlateCarree())\n",
    "\n",
    "mask = (storm_df['date'] <= selected_time) & (storm_df['date'] > (selected_time - timedelta(hours=  storm_time_bounds)))\n",
    "storm_position = storm_df.loc[mask].tail(1)\n",
    "map_storm_position= ax.scatter(x=storm_position['lon'].iloc[0], y=storm_position['lat'].iloc[0], c= 'red', \n",
    "           marker = '*', s=600, transform=ccrs.PlateCarree())\n",
    "\n",
    "map_stations = ax.scatter(x=matching_points['longitude (degrees_east)'], y=matching_points['latitude (degrees_north)'],  \n",
    "           c=matching_points[focus_variable + \" \" + var_units], cmap =  color_map, marker = 'v', s=200, transform=ccrs.PlateCarree(), alpha= 1,\n",
    "           edgecolors='black')\n",
    "\n",
    "# TODO: Fix issue where colormap sets to minimum instead of at 0 (or vice versa)\n",
    "\n",
    "norm = colors.Normalize(matching_points[focus_variable + \" \" + var_units].min(), \n",
    "                        matching_points[focus_variable + \" \" + var_units].max())\n",
    "fig.colorbar(cm.ScalarMappable(norm= norm, cmap=color_map), ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create plots  for matching datasets\n",
    "e = ERDDAP(\n",
    "    server=\"https://cioosatlantic.ca/erddap\", \n",
    "    protocol=\"tabledap\",\n",
    "    response=\"csv\",\n",
    ")\n",
    "\n",
    "for dataset in datasets:\n",
    "\n",
    "    # Still keeps generating graph after dataset fails for some reason?\n",
    "\n",
    "    e.dataset_id = dataset\n",
    "\n",
    "    e.constraints = {\n",
    "        \"time>=\": start_date - timedelta(days = 2),\n",
    "        \"time<=\": end_date + timedelta(days = 2)\n",
    "    }\n",
    "\n",
    "    e.variables = [\"time\", \"longitude\", \"latitude\", focus_variable]\n",
    "\n",
    "    try:\n",
    "        buoy_data= e.to_pandas(\n",
    "            parse_dates=True,\n",
    "        ).dropna()\n",
    "        buoy_data.plot(x='time (UTC)', y=focus_variable + ' ' + var_units, title=dataset)\n",
    "    except:\n",
    "        print(\"Data does not exist for %s\", dataset) \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hurricane",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "71842d5228d3008246279ae0164097a1e8052b15aaf1b5deabef9a767d14ca29"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
